{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb0m5hKfMt67rC+UOqavzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byundonghwan/maching_learning_basic/blob/main/%EC%95%99%EC%83%81%EB%B8%94_%EB%AA%A8%EB%8D%B8_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZFxkAVt9gWa",
        "outputId": "27600592-1bf3-4c5d-82bb-c4de180c2d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression  :  1.0\n",
            "RandomForestClassifier  :  0.9333333333333333\n",
            "SVC  :  1.0\n",
            "VotingClassifier  :  1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split # 훈련 데이터와 데스트 데이터 분류 라이브러리\n",
        "\n",
        "# 데이터셋 로드\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # 꽃잎의 길이, 너비\n",
        "Y = iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size =0.3, random_state = 2021, shuffle = True)\n",
        "\n",
        "# 약한 학습기 구축\n",
        "log_model = LogisticRegression()\n",
        "rnd_model = RandomForestClassifier()\n",
        "svm_model = SVC()\n",
        "\n",
        "# 앙상블 모델 구축\n",
        "voting_model = VotingClassifier(\n",
        "    estimators = [('lr', log_model), ('rf', rnd_model), ('svc', svm_model)], # 3개의 약한 학습기\n",
        "    voting = 'hard' # 직접 투표 (hard voting)\n",
        ")\n",
        "\n",
        "# 앙상블 모델 학습\n",
        "voting_model.fit(x_train, y_train)\n",
        "\n",
        "# 모델 비교\n",
        "for model in (log_model, rnd_model, svm_model, voting_model):\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  print(model.__class__.__name__, \" : \", accuracy_score(y_test, y_pred))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사이킷런의 배깅과 페이스팅\n",
        "# 배깅(중복 허용 샘플링)을 하다보면 평균적으로 훈련 샘플의 약 63%정도만 추출되고 \n",
        "# 나머지 약 37%는 추출되지 않고, 이렇게 추출되지 않은 샘플들을 oob(out-of-bag)샘플이라고 부른다.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_model = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), # 약한 학습기(결정 트리)\n",
        "    n_estimators = 500, # 약한 학습기(결정 트리) 500개 생성.\n",
        "    max_samples = 0.05, # 0.0 ~ 1.0 사이 실수 선택 \n",
        "    bootstrap = True, # True : 배깅, False : 페이스팅\n",
        "    n_jobs = -1, # 훈련과 예측에 사용할 Cpu 코어 수( -1: 가용한 모든 코어 사용.)\n",
        "    oob_score = True # oob 평가를 위해 True를 지정한다. \n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "bag_model.fit(x_train, y_train)\n",
        "\n",
        "# 모델 예측\n",
        "y_pred = bag_model.predict(x_test)\n",
        "\n",
        "# 모델 평가\n",
        "print(bag_model.__class__.__name__, \" : \", accuracy_score(y_test, y_pred))\n",
        "print('oob_score : ', bag_model.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqUI6REa-bmp",
        "outputId": "6a806405-e4a3-4ec6-b9c1-a8b90e05fd41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaggingClassifier  :  0.9777777777777777\n",
            "oob_score :  0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 포레스트\n",
        "# 랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다.\n",
        "# 그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, RandomForestClassifier를 사용할 수 있다.\n",
        "# 랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, \n",
        "# 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택하여 무작위성을 더 가지게 된다.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 랜덤 포레스트 모델 구축.\n",
        "rnd_model = RandomForestClassifier(\n",
        "    n_estimators = 500, # 예측기 500개 \n",
        "    max_leaf_nodes = 16, # 자식노드의 최대 개수\n",
        "    n_jobs = -1 # CPU 코어 구동 개수\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "rnd_model.fit(x_train, y_train)\n",
        "\n",
        "# 모델 예측\n",
        "y_pred_rf = rnd_model.predict(x_test)\n",
        "\n",
        "# 모델 평가\n",
        "print(\"rnd_model : \", accuracy_score(y_pred_rf, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mPYZ6zL_nds",
        "outputId": "4324400d-4026-4653-c0c2-07f5a35dd12b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnd_model :  0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤포레스트는 성능이 좋다는 장점말고, 특성의 상대적 중요도를 측정하기 쉽다.(트리기반 모델은 특성 중요도 제공)\n",
        "# 사이킷런에서는 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는지 확인하여 특성 중요도를 측정하고, 훈련이 끝나고 난 뒤에 특성마다 자동으로 점수를 계산하고 저장한다.\n",
        "# 저장된 값은 featureimportances 변수에 저장되어 있다.\n",
        "\n",
        "# 데이터셋 정의\n",
        "x = iris.data[:, :]\n",
        "y = iris.target\n",
        "\n",
        "# 모델 구축\n",
        "rnd_model = RandomForestClassifier(\n",
        "    n_estimators = 500, # 예측기 500개\n",
        "    n_jobs = -1 # 가용 cpu 갯수\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "rnd_model.fit(x, y)\n",
        "\n",
        "# 특성 중요도 확인 (전체 특성 중요도 합 : 1)\n",
        "for feature_name, feature_imp in zip(iris['feature_names'], rnd_model.feature_importances_):\n",
        "  print(feature_name, ' : ', feature_imp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx8tKllwFsri",
        "outputId": "682226f2-e325-4535-907d-cd807032369f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm)  :  0.09602191822022164\n",
            "sepal width (cm)  :  0.021563477435701948\n",
            "petal length (cm)  :  0.43747433491687676\n",
            "petal width (cm)  :  0.4449402694271996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부스팅\n",
        "# 부스팅이란, 약한 학습기를 여러 개들을 서로 연결하고 보완해가면서 더욱 강한 학습기를 만드는 앙상블 방법이다.\n",
        "\n",
        "# 아다부스트의 아이디어는 이전 예측기가 과소적합되었던 훈련 샘플의 가중치를 업데이트 하면서 더욱 높이는 것이다.\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 아다부스트 모델 구축.\n",
        "# 아다부스트 학습기 : Decision Tree (max_depth = 1) 사용\n",
        "# 학습기 개수(n_estimators) : 200개\n",
        "\n",
        "ada_model = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth = 1),\n",
        "    n_estimators = 200, \n",
        "    algorithm = 'SAMME.R',\n",
        "    learning_rate = 0.5\n",
        ")\n",
        "\n",
        "ada_model.fit(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFZ2MVPJG0pl",
        "outputId": "b1367bb8-b6a9-46ec-cc25-ef2ca2a48ebe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래디언트 부스팅은 아다부스팅과 비슷하게 학습 샘플에 대해 오차를 보정하면서 순차적으로 예측기를 추가 한다.\n",
        "# 하지만 차이점은 아다부스트처럼 각 학습 샘플에 대한 가중치를 업데이트 하는 대신, 이전 예측기가 만든 잔차(residual error)에 새로운 예측기를 학습시키는 것이다.\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# 결정트리 (max_depth = 3) 모델 구축 및 학습\n",
        "tree_reg_model_1 = DecisionTreeRegressor(max_depth = 3)\n",
        "tree_reg_model_1.fit(x, y)\n",
        "\n",
        "# 첫 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습.\n",
        "residual_1 = y - tree_reg_model_1.predict(x)\n",
        "tree_reg_model_2 = DecisionTreeRegressor(max_depth = 3)\n",
        "tree_reg_model_2.fit(x, residual_1)\n",
        "\n",
        "# 두 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습.\n",
        "residual_2 = y - tree_reg_model_2.predict(x)\n",
        "tree_reg_model_3 = DecisionTreeRegressor(max_depth = 3)\n",
        "tree_reg_model_3.fit(x, residual_2)\n",
        "\n",
        "# 새로운 데이터를 세 개의 트리를 포함한 앙상블 모델로 예측.\n",
        "x_new = [[1.4, 0.2]]\n",
        "#prediction = sum(tree.predict(x_new) for tree in [tree_reg_model_1, tree_reg_model_2, tree_reg_model_3])\n",
        "#prediction\n"
      ],
      "metadata": {
        "id": "u0PFoeIAH8r1"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}
